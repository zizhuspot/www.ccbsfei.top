---
title: DDPM æ¦‚ç‡æ¨¡å‹ä»0åˆ°1æ·±åº¦è§£æ
date: 2023-08-01 17:21:00
categories:
  - AIç»˜ç”»
tags:
  - æ–‡ç”Ÿå›¾
  - aiç”»å›¾
  - DDPM
  - æ‰©æ•£æ¨¡å‹
  - å»å™ªè‡ªç¼–ç å™¨ 
description: å‘ç°äº†æ‰©æ•£æ¨¡å‹ä¸ç”¨äºåŸ¹è®­é©¬å°”ç§‘å¤«è¿‡ç¨‹çš„å˜åˆ†æ¨ç†ã€å»å™ªåˆ†æ•°åŒ¹é…ã€é€€ç«LangevinåŠ¨åŠ›å­¦ã€è‡ªå›å½’æ¨¡å‹ä»¥åŠæ¸è¿›æœ‰æŸå‹ç¼©ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ I have discovered a correlation between diffusion models and variational inference for training Markov processes, denoising score matching, Langevin dynamics with simulated annealing, autoregressive models, and asymptotic lossy compression.  
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/DDPM.png
---


## æ‘˜è¦å’Œä»‹ç»

- ä½œè€…ä½¿ç”¨äº†æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°åŒ–é©¬å°”å¯å¤«é“¾ï¼Œé€šè¿‡å˜åˆ†æ¨ç†è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸ªæ¨¡å‹å¯ä»¥åœ¨æœ‰é™çš„æ—¶é—´å†…ç”Ÿæˆä¸æ•°æ®åŒ¹é…çš„æ ·æœ¬ã€‚ä¸ºäº†å­¦ä¹ è¿™ä¸ªé©¬å°”å¯å¤«é“¾çš„è½¬ç§»è§„åˆ™ï¼Œä½œè€…åå‘æ“ä½œæ‰©æ•£è¿‡ç¨‹ï¼Œè¿™ä¸ªè¿‡ç¨‹å®é™…ä¸Šæ˜¯å°†å™ªå£°é€æ¸æ·»åŠ åˆ°æ•°æ®ä¸­ï¼Œç›´åˆ°ä¿¡å·å˜å¾—ä¸æ¸…æ™°ã€‚ä½œè€…å°†è¿™ä¸ªé€†å‘çš„æ‰©æ•£è¿‡ç¨‹å»ºæ¨¡ä¸ºæ¡ä»¶é«˜æ–¯åˆ†å¸ƒï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥ç”±ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå‚æ•°åŒ–ã€‚

- æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ç±»ä¼¼äºæ¸è¿›å¼çš„è§£ç ï¼Œè¿™æ„å‘³ç€å®ƒé€æ­¥ç”Ÿæˆæ ·æœ¬ï¼Œå°±åƒæŒ‰é¡ºåºè§£ç ä½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½œè€…èƒ½å¤Ÿè·å¾—é«˜è´¨é‡çš„å›¾åƒåˆæˆç»“æœï¼Œå¹¶ä¸”åœ¨è¯„ä¼°ä¸Šå–å¾—äº†è‰¯å¥½çš„åˆ†æ•°ï¼ŒåŒ…æ‹¬Inception scoreåˆ†æ•°å’ŒFID åˆ†æ•°ã€‚è¿™è¡¨æ˜è¿™ç§æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢å…·æœ‰å¾ˆé«˜çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ã€‚


- æ‰©æ•£æ¨¡å‹ åŒ…å«çš„ä¸¤ä¸ªè¿‡ç¨‹
- å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼š
- åå‘ç”Ÿæˆè¿‡ç¨‹

The author used a diffusion probability model, which is a parameterized Markov chain trained through variational inference. This model can generate samples that match the data in a finite amount of time. To learn the transition rules of this Markov chain, the author reversed the diffusion process by gradually adding noise to the data until the signal becomes unclear. The author modeled this reverse diffusion process as a conditional Gaussian distribution, which enabled it to be parameterized by a simple neural network.

The sampling process of the diffusion model is similar to gradual decoding, meaning that it generates samples step by step, like decoding bits in order. In this way, the author was able to obtain high-quality image synthesis results and achieve good scores on evaluations, including Inception score and FID score. This indicates that this method has high potential for generating images, especially when dealing with large datasets.

The two processes included in the diffusion model are:

Forward Diffusion Process: This is the process of gradually transforming a random variable into a Gaussian distribution over time. At each time step, the current variable is mixed with a small amount of noise to create the next variable. This gradually increases the noise in the distribution until it matches the target noise level.

Reverse Diffusion Process: This is the process of gradually transforming a Gaussian distribution back into the original variable. This is done by adding noise to the Gaussian distribution at each time step and removing the noise using an inference network. The inference network is trained to approximate the reverse process of the forward diffusion process and can be used to generate samples from the Gaussian distribution.



![æ‰©æ•£æ¨¡å‹](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175755.png)


## èƒŒæ™¯

- å…¬å¼1ï¼šé€†å‘è¿‡ç¨‹ï¼Œè¢«å®šä¹‰æˆé©¬å°”ç§‘å¤«é“¾ï¼Œä»p(ğ‘¥_ğ‘‡ )=ğ‘(ğ‘¥_ğ‘‡:0,I)
- å¼€å§‹å­¦ä¹ é«˜æ–¯è½¬æ¢ã€‚

Equation 1: The reverse process is defined as a Markov chain starting from p(x_t) = N(x_t; 0, I).
To learn the Gaussian transition.


![èƒŒæ™¯1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175844.png)

- å…¬å¼2 ï¼šæ‰©æ•£æ¨¡å‹å’Œå…¶ä»–éšå˜é‡æ¨¡å‹åŒºåˆ«æ˜¯è¿‘ä¼¼åéªŒ(å‰å‘/æ‰©æ•£è¿‡ç¨‹)è¢«å®šä¹‰æˆé©¬å°”ç§‘å¤«é“¾ï¼Œæ ¹æ®å˜åˆ†scheduleé€æ­¥å‘æ•°æ®ä¸­æ·»åŠ å™ªå£°ã€‚

![å…¬å¼2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175909.png)

- å…¬å¼3 ï¼šä¼˜åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„å˜åˆ†ç•Œé™è¿›è¡Œè®­ç»ƒ

![å…¬å¼3](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175936.png)

- å…¬å¼4 ï¼šå‰å‘è¿‡ç¨‹çš„æ˜¾è‘—ç‰¹æ€§æ˜¯å…è®¸ä»¥é—­å¼åœ¨ä»»æ„æ—¶é—´æ­¥té‡‡æ ·xt
  ğ›¼_tâ‰”1-ğ›½_ğ‘¡ , Â¯(ğ›¼_ğ‘¡ ):=âˆ2_(ğ‘ =1)^ğ‘¡â–’ğ›¼_ğ‘  

![å…¬å¼4](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729175959.png)

- å…¬å¼5,6,7 ï¼š ä½¿ç”¨SGDä¼˜åŒ–Lçš„éšæœºé¡¹æ¥è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚é€šè¿‡é™ä½æ–¹å·®æ¥è¿›ä¸€æ­¥æå‡ï¼Œé‡å†™Lå¦‚å…¬å¼äº”ï¼Œå…¬å¼äº”é€šè¿‡KLæ•£åº¦å»è¡¡é‡p_ğœƒ  ã€–(ğ‘¥ã€—_(ğ‘¡âˆ’1) |ğ‘¥_ğ‘¡)å’Œå‰å‘è¿‡ç¨‹åéªŒï¼Œå½“æ¡ä»¶æ˜¯å…¬å¼6ï¼Œ7ã€‚æ‰€æœ‰çš„KLæ•£åº¦éƒ½æ˜¯é«˜æ–¯ä¹‹é—´çš„å¯¹æ¯”ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨RaoBlackwellized çš„é—­å¼è¡¨è¾¾å¼è®¡ç®—ä»£æ›¿é«˜æ–¹å·®çš„Monte Carloä¼°è®¡ã€‚

![å…¬å¼5,6,7 ](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180030.png)

![å…¬å¼1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180041.png)


## æ‰©æ•£æ¨¡å‹å’Œå»å™ªè‡ªç¼–ç å™¨

- å…¬å¼8ï¼š æ‰©æ•£æ¨¡å¼çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå—é™çš„æ½œå˜é‡æ¨¡å‹ï¼Œä½†ä»–ä»¬å…è®¸å¾ˆå¤§çš„è‡ªç”±åº¦ã€‚å¿…é¡»é€‰æ‹©æ­£å‘è¿‡ç¨‹æ–¹å·®ğ›½_tï¼Œæ¨¡å‹æ¶æ„å’Œåå‘è¿‡ç¨‹çš„é«˜æ–¯åˆ†å¸ƒå‚æ•°åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†æ‰©æ•£æ¨¡å‹å’Œå»å™ªåˆ†æ•°åŒ¹é…ä¹‹é—´çš„è¿æ¥æ¥ä¸ºæ‰©æ•£æ¨¡å‹åšä¸€ä¸ªç®€å•çš„åŠ æƒçš„å˜åˆ†ç•Œé™ç›®æ ‡ã€‚
- å‰å‘è¿‡ç¨‹å’ŒLTï¼šLTæ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥å¿½ç•¥

Equation 8: The diffusion model appears to be a variant of a latent variable model with a lot of freedom. It is necessary to choose the variance of the forward process Î±_t, the model architecture, and the parameterization of the Gaussian distribution for the reverse process. We established a connection between the diffusion model and denoising score matching to provide a simple weighted variational bound objective for the diffusion model.
Forward Process and LT: LT is a constant that can be ignored during training.

- é€†å‘è¿‡ç¨‹å’Œğ¿_(1:ğ‘‡âˆ’1)ï¼šè®¾ç½®âˆ‘2_ğœƒâ–’ã€–(ğ‘¥_ğ‘¡,ğ‘¡)ã€—, ğ‘¥_0~ğ‘(0,1),ğœ‡_ğœƒ (ğ‘¥_ğ‘¡,ğ‘¡),         ,p_ğœƒ(ğ‘¥_(ğ‘¡âˆ’1) |ğ‘¥_ğ‘¡)=Nã€–(ğ‘¥ã€—_(ğ‘¡âˆ’1);ğœ‡_ğœƒ (ğ‘¥_ğ‘¡,ğ‘¡),ğ›¿_ğ‘¡^2 I)ï¼ŒL_(tâˆ’1) å…¬å¼é‡å†™å¦‚ä¸‹ï¼š


![æ‰©æ•£æ¨¡å‹å’Œå»å™ªè‡ªç¼–ç å™¨](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180212.png)


- å…¬å¼9,10,11ï¼šCæ˜¯ä¸ä¾èµ–ğœƒçš„å¸¸æ•°ï¼Œå¯¹ğœ‡_ğœƒçš„å‚æ•°åŒ–å°±æ˜¯é¢„æµ‹(ğœ‡_t )Â Ìƒï¼Œæ ¹æ®ç­‰å¼4é‡æ–°å‚æ•°åŒ–æ¥æ‰©å±•å…¬å¼8ï¼Œä½¿ç”¨å‰å‘è¿‡ç¨‹åéªŒå…¬å¼7ã€‚
- ğœ‡_ğœƒ (ğ‘¥_ğ‘¡,ğ‘¡)çš„å‚æ•°åŒ–å¦‚å…¬å¼11

![å…¬å¼9,10,11](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729180240.png)

æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒé€†å‘è¿‡ç¨‹å‡å€¼å‡½æ•°é€¼è¿‘å™¨ğœ‡_ğœƒå»é¢„æµ‹(ğœ‡_t )Â Ìƒï¼Œæˆ–è€…ä¿®æ”¹å®ƒçš„å‚æ•°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¢„æµ‹âˆˆã€‚
âˆˆ_ğœƒåœ¨Denoising Score Matching é‡Œé¢æ˜¯ä¼°è®¡çš„æ¢¯åº¦ï¼Œè€Œå™ªå£°âˆˆå°±æ˜¯å¸¦å™ªå£°æ•°æ®åˆ†å¸ƒçš„score,å³æ¦‚ç‡å¯†åº¦æ¢¯åº¦å€¼ã€‚

![ç®—æ³•1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222453.png)

![ç®—æ³•2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222518.png)


![æŠ½æ ·](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222545.png)

![æŠ½æ ·2](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222600.png)


è¿™ä¸ªç ”ç©¶ä¸­ï¼Œå›¾åƒæ•°æ®è¢«çº¿æ€§ç¼©æ”¾åˆ°èŒƒå›´[-1,1]ï¼Œç”±æ•´æ•°ç»„æˆã€‚è¿™ç¡®ä¿äº†ç¥ç»ç½‘ç»œçš„åå‘è¿‡ç¨‹ä»¥æ ‡å‡†å…ˆéªŒåˆ†å¸ƒp(ğ‘¥_ğ‘‡)å¼€å§‹ã€‚ä¸ºäº†è®¡ç®—ç¦»æ•£çš„å¯¹æ•°ä¼¼ç„¶ï¼Œä½œè€…å°†åå‘è¿‡ç¨‹çš„æœ€åä¸€æ­¥è®¾ç½®ä¸ºä»é«˜æ–¯åˆ†å¸ƒN(ğ‘¥_0;ğœ‡_ğœƒ(ğ‘¥_1,1),ğ›¿_1^2 I)å¯¼å‡ºçš„ç¦»æ•£è§£ç å™¨ã€‚

è¿™é‡Œçš„Dä»£è¡¨æ•°æ®çš„ç»´åº¦ï¼Œiä»£è¡¨æå–ç»´åº¦ã€‚ä½œè€…çš„é€‰æ‹©ç¡®ä¿äº†å˜åˆ†ç•Œé™æ˜¯ç¦»æ•£æ•°æ®çš„æ— æŸç é•¿ï¼Œè€Œæ— éœ€å‘æ•°æ®æ·»åŠ å™ªå£°æˆ–åˆå¹¶ç¼©æ”¾æ“ä½œçš„é›…å¯æ¯”çŸ©é˜µåˆ°å¯¹æ•°ä¼¼ç„¶ä¸­ã€‚

ä½œè€…è¿˜æŒ‡å‡ºï¼Œæ ¹æ®é€†å‘è¿‡ç¨‹å’Œè§£ç å™¨çš„å®šä¹‰ï¼Œå˜åˆ†ç•Œé™å…³äºğœƒæ˜¯å¯å¾®çš„ã€‚å› æ­¤ï¼Œåœ¨ä¸‹é¢çš„å˜åˆ†ç•Œé™ä¸Šè¿›è¡Œè®­ç»ƒæœ‰åŠ©äºæé«˜æ ·æœ¬è´¨é‡ã€‚

ä½œè€…è¿˜ç®€åŒ–äº†ç›®æ ‡å‡½æ•°ï¼Œå»é™¤äº†å…¬å¼ä¸­çš„æƒé‡å› å­ã€‚è¿™ä½¿å¾—åŠ æƒå˜åˆ†ç•Œé™æ›´åŠ å¼ºè°ƒé‡å»ºçš„å„ä¸ªæ–¹é¢ï¼Œç›¸å¯¹äºæ ‡å‡†çš„å˜åˆ†ç•Œé™ã€‚

æœ€åï¼Œä½œè€…æŒ‡å‡ºï¼Œç®€åŒ–ç›®æ ‡å‡½æ•°çš„æ‰©æ•£æ¨¡å‹ä¼šå‡å°‘ä¸å°äºtçš„å„é¡¹æŸå¤±ç›¸å¯¹åº”çš„æƒé‡ã€‚è¿™å¯ä»¥ä½¿ç½‘ç»œåœ¨æ›´å¤§çš„Té¡¹ä¸­æ›´åŠ ä¸“æ³¨äºæ›´å…·æŒ‘æˆ˜æ€§çš„å»å™ªä»»åŠ¡ã€‚

Here, D represents the dimension of the data, and i represents the extracted dimension. The author's choice ensures that the variational bound is the lossless code length of the discrete data without the need to add noise to the data or incorporate the Jacobian matrix of the scaling operation into the log-likelihood.

The author also points out that according to the definition of the reverse process and the decoder, the variational bound is differentiable with respect to Î¸. Therefore, training on this variational bound can help improve sample quality.

The author also simplified the objective function by removing the weighting factor in the formula, which makes the weighted variational bound more emphasizing on various aspects of reconstruction compared to the standard variational bound.

Finally, the author points out that simplifying the objective function of the diffusion model reduces the weight corresponding to the loss terms for smaller t values. This allows the network to focus more on the more challenging denoising tasks among a larger number of T terms.



## å®éªŒ

![å®éªŒ1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729222958.png)


è¡¨1åˆ—å‡ºäº†åœ¨CIFAR10æ•°æ®é›†ä¸Šçš„Inception Scoreã€FIDåˆ†æ•°å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚å½“FIDåˆ†



### Progressive coding 



è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„å·®è·æœ€å¤šä¸ºæ¯ç»´0.03ä½ï¼Œè¡¨æ˜æ‰©æ•£æ¨¡å‹æ²¡æœ‰è¿‡åº¦æ‹Ÿåˆã€‚
æ‰©æ•£æ¨¡å‹å…·æœ‰æ„Ÿåº”åå·®ï¼Œä½¿å…¶æˆä¸ºå‡ºè‰²çš„æœ‰æŸå‹ç¼©å™¨ï¼Œå› ä¸ºæ ·æœ¬è´¨é‡å¾ˆé«˜ã€‚
æ¸è¿›å¼æœ‰æŸå‹ç¼©å¯ä»¥é€šè¿‡å¼•å…¥åæ˜ æ–¹ç¨‹å¼å½¢å¼çš„æ¸è¿›å¼æœ‰æŸä»£ç æ¥è¿›ä¸€æ­¥ç ”ç©¶æ¨¡å‹çš„é€Ÿç‡å¤±çœŸè¡Œä¸ºã€‚

The maximum difference between training and testing is 0.03 bits per dimension, indicating that the diffusion model has not overfitted.
The diffusion model has inductive bias that makes it a excellent lossy compressor because of the high sample quality.
æ¸è¿›å¼æœ‰æŸå‹ç¼© can be further studied by introducing an iterative equation form of the lossy code.

![å›¾äº”](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223119.png)

å›¾äº”æ˜¾ç¤ºäº†:é€†å‘è¿‡ç¨‹ä¸­æ—¶é—´ã€é€Ÿç‡å’Œå¤±çœŸç‡çš„å…³ç³»

åœ¨é€Ÿç‡ç•¸å˜å›¾çš„ä½é€Ÿç‡åŒºåŸŸï¼Œç•¸å˜æ€¥å‰§å‡å°; è¿™è¡¨æ˜å¤§éƒ¨åˆ†æ¯”ç‰¹ç¡®å®è¢«åˆ†é…ç»™äº†éš¾ä»¥å¯Ÿè§‰çš„å¤±çœŸã€‚

### Progressive generation

- æ— æ¡ä»¶CIFAR 10æ¸è¿›å¼ç”Ÿæˆã€‚ä»å™ªå£°å›¾åƒåˆ°æ¸…æ™°çš„å›¾åƒè¿‡ç¨‹ã€‚
- æ‰©å±•æ ·æœ¬å’Œæ ·æœ¬è´¨é‡è¯„ä¼°åœ¨å›¾10å’Œ14ä¸­

![generation 1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223215.png)


æ˜¾ç¤ºäº†éšæœºé¢„æµ‹x_0~p_0 ã€–(xã€—_0|x_t),å¯¹äºä¸åŒçš„tï¼Œx_t è¢«å†»ç»“ã€‚å½“tå¾ˆå°æ—¶ï¼Œç»†èŠ‚è¢«ä¿ç•™ä¸‹æ¥ï¼Œtå¾ˆå¤§æ—¶ï¼Œå¤§çš„ç‰¹å¾è¢«ä¿ç•™ä¸‹æ¥ã€‚
å³ä¸‹è§’æ˜¯x_tï¼Œå…¶ä»– æ¥è‡ªäºp_t ã€–(xã€—_0|x_t)çš„é‡‡æ ·ã€‚

![æ˜¾ç¤º1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729223325.png)


## ç›¸å…³å·¥ä½œ

ç ”ç©¶é€šè¿‡Îµ-é¢„æµ‹åå‘è¿‡ç¨‹å‚æ•°åŒ–ï¼Œå»ºç«‹äº†æ‰©æ•£æ¨¡å‹ä¸LangevinåŠ¨åŠ›å­¦ä¹‹é—´çš„è”ç³»ï¼Œä¸ºé«˜è´¨é‡çš„å›¾åƒåˆæˆå’Œé‡‡æ ·æä¾›äº†æ–°çš„æ–¹æ³•ï¼Œå¹¶åœ¨è¯„ä¼°ä¸­å¯¹èƒ½é‡æ¨¡å‹é¢†åŸŸäº§ç”Ÿäº†ç§¯æå½±å“ã€‚

The research established a connection between the diffusion model and Langevin dynamics by parameterizing the reverse process with Îµ-prediction, providing a new approach for high-quality image synthesis and sampling, and positively impacting the field of energy-based models in evaluation.

## ç»“è®º


ç ”ç©¶æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹ä¸å˜åˆ†æ¨ç†ã€å»å™ªåˆ†æ•°åŒ¹é…ã€LangevinåŠ¨æ€ã€è‡ªå›å½’æ¨¡å‹å’Œæœ‰æŸå‹ç¼©ä¹‹é—´çš„è”ç³»ï¼Œå¯¹ä¸åŒæ•°æ®æ¨¡æ€å’Œç”Ÿæˆæ¨¡å‹å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå¹¶å¼ºè°ƒäº†ç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨æ»¥ç”¨é£é™©ã€‚

The research has revealed the connections between diffusion models and variational inference, denoising score matching, Langevin dynamics, autoregressive models, and lossy compression, which have potential applications for different data modalities and generative models. It also emphasizes the potential risk of misuse in generative models.

## é™„å½•ABCDE

- ç¥ç»ç½‘ç»œæ¶æ„éµå¾ªPixelCNN + + çš„ä¸»å¹²ï¼Œ32*32æ¨¡å‹ä½¿ç”¨4ç§ç‰¹å¾æ˜ å°„åˆ†è¾¨ç‡ï¼Œ256*256æ¨¡å‹ä½¿ç”¨6ç§ã€‚ä½œè€…çš„CIFAR10 æ¨¡å‹æœ‰3570 ä¸‡ä¸ªå‚æ•°ï¼ŒLSUN å’ŒCelebA-HQ æ¨¡å‹æœ‰114 ä¸‡ä¸ªå‚æ•°ã€‚ä½œè€…è¿˜é€šè¿‡å¢åŠ æ»¤æ³¢å™¨è®¡æ•°è®­ç»ƒäº†LSUN Bedroomæ¨¡å‹çš„è¾ƒå¤§å˜ä½“ï¼Œå‚æ•°çº¦ä¸º256 ä¸‡ã€‚
- ä½œè€…ä½¿ç”¨TPU v3-8 ï¼ˆç±»ä¼¼äº8 ä¸ªv100 GPU ï¼‰è¿›è¡Œå®éªŒã€‚CIFAR æ¨¡å‹ä»¥æ¯ç§’21 æ­¥çš„é€Ÿåº¦è®­ç»ƒï¼Œbatchsize=128 ï¼ˆ10.6 å°æ—¶è®­ç»ƒåˆ°800k æ­¥éª¤ï¼‰ï¼Œé‡‡æ ·ä¸€æ‰¹256 ä¸ªå›¾åƒéœ€è¦17 ç§’ã€‚
- ä½œè€…çš„CelebA-HQ / LSUN ï¼ˆã€–256ã€—^2ï¼‰æ¨¡å‹åœ¨batch size=64 æ—¶ä»¥æ¯ç§’2.2 æ­¥çš„é€Ÿåº¦è®­ç»ƒï¼Œé‡‡æ ·128 ä¸ªå›¾åƒéœ€è¦300 ç§’ã€‚ä½œè€…åœ¨CelebA-HQ ä¸Šè®­ç»ƒäº†0.5M æ­¥ï¼ŒLSUN è®­ç»ƒäº†2.4M æ­¥ï¼ŒLSUN Cat è®­ç»ƒäº†1.8M æ­¥ï¼ŒLSUN Church è®­ç»ƒäº†1.2M æ­¥ï¼Œè¾ƒå¤§çš„LSUN Bedroomæ¨¡å‹è®­ç»ƒäº†1.15M æ­¥ã€‚

The neural network architecture follows the PixelCNN++ backbone. The 32x32 model uses 4 feature map resolutions, while the 256x256 model uses 6. The author's CIFAR10 model has 35.7 million parameters, while the LSUN and CelebA-HQ models have 1.14 million parameters. The author also trained a larger variant of the LSUN Bedroom model with around 2.56 million parameters by increasing the filter count.
The author used TPU v3-8 (similar to 8 v100 GPUs) for the experiments. The CIFAR model trains at a speed of 21 steps per second with a batch size of 128 (10.6 hours to train to 800k steps), and it takes 17 seconds to sample a batch of 256 images.
The author's CelebA-HQ/LSUN (ã€–256ã€—^2) model trains at a speed of 2.2 steps per second with a batch size of 64, and it takes 300 seconds to sample 128 images. The author trained for 0.5M steps on CelebA-HQ, 2.4M steps on LSUN, 1.8M steps on LSUN Cat, 1.2M steps on LSUN Church, and 1.15M steps on the larger LSUN Bedroom model.




