---
title: InstructGPT æ¨¡å‹æ·±åº¦è§£å¯†
date: 2023-07-27 11:30:00
categories:
  - å¤§æ¨¡å‹
tags:
  - GPT1
  - GPT2
  - GPT3
  - InstructGPT
  - å¼ºåŒ–å­¦ä¹ 
description: ä½¿ç”¨PPOæ¥å¾®è°ƒSFTæ¨¡å‹ã€‚è¾“å…¥ä¸€ä¸ªpromptæœŸæœ›å¾—åˆ°ä¸€ä¸ªè¾“å‡ºã€‚ç»™å®šä¸€ä¸ªpromptå’Œresponseï¼Œç”Ÿæˆå¥–åŠ±åˆ†æ•°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¢åŠ äº†KLæ•£åº¦é™ä½å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªæ¨¡å‹ä¸ºPPOã€‚ä½œè€…æŠŠé¢„è®­ç»ƒçš„æ¢¯åº¦åŠ å…¥åˆ°PPOçš„æ¢¯åº¦ä¸­ï¼Œä¸ºäº†ç¼“å’Œæ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸­çš„æ€§èƒ½æŸå¤±ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªæ¨¡å‹ä¸ºPPO-ptxã€‚
cover: https://cdn.jsdelivr.net/gh/1oscar/image_house@main/InstructGPT.jpg
---

## æ‘˜è¦&ä»‹ç»

### ChatGPT å‘å±•æ—¶é—´çº¿

![ChatGPTå‘å±•æ—¶é—´çº¿](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092614.png)

å¢å¤§è¯­è¨€æ¨¡å‹å¹¶ä¸ä¸€å®šä¿è¯å…¶è¾“å‡ºä¸äººç±»æ„å›¾ä¸€è‡´ã€‚è¾ƒå¤§çš„è¯­è¨€æ¨¡å‹å¯èƒ½ç”Ÿæˆä¸çœŸå®ã€æœ‰å®³æˆ–æ— æ•ˆçš„ç­”æ¡ˆã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å¯èƒ½ä¸ç”¨æˆ·çš„é¢„æœŸä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œä½œè€…é‡‡å–äº†ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨ç”¨æˆ·çš„åé¦ˆæ¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚

Increasing the size of a language model does not necessarily guarantee that its output will align with human intent. Larger language models can generate unreal, harmful, or ineffective responses. In other words, the model may be inconsistent with user expectations. In this study, the authors employed a fine-tuning approach using user feedback to enhance model performance.

é¦–å…ˆï¼Œä»–ä»¬ä½¿ç”¨ä¸€ç»„ç»è¿‡äººå·¥æ ‡æ³¨çš„æç¤ºè¯æ¥è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åæ”¶é›†äº†æ¨¡å‹ç”Ÿæˆçš„æœ‰åºæ•°æ®é›†ã€‚æ¥ç€ï¼Œä»–ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œä»äººç±»åé¦ˆä¸­è¿›ä¸€æ­¥å¾®è°ƒç»è¿‡ç›‘ç£å­¦ä¹ çš„æ¨¡å‹ã€‚è¿™ä¸€ç»è¿‡å¾®è°ƒçš„æ¨¡å‹è¢«ç§°ä¸º"instruct GPT"ã€‚

First, they used a set of manually annotated prompt words for supervised fine-tuning and collected an ordered dataset of model-generated responses. Then, they further fine-tuned the model using Reinforcement Learning from Human Feedback (RLHF) methods based on human feedback. This fine-tuned model is referred to as "instruct GPT."

äººç±»è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºæ‹¥æœ‰175Bå‚æ•°çš„gpt-3æ¨¡å‹ï¼Œæ‹¥æœ‰1.3Bå‚æ•°çš„"instruct GPT"æ¨¡å‹çš„è¾“å‡ºæ›´åŠ ä¼˜è¶Šã€‚å°½ç®¡"instruct GPT"æ¨¡å‹å¶å°”ä¼šå‡ºç°ä¸€äº›ç®€å•çš„é”™è¯¯ï¼Œä½†è¿™ä¸ªç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨äººç±»åé¦ˆè¿›è¡Œå¾®è°ƒæ˜¯æœç€ä½¿æ¨¡å‹è¾“å‡ºä¸äººç±»æ„å›¾ä¸€è‡´çš„æ­£ç¡®æ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ã€‚

Human evaluation results show that the 1.3-billion-parameter "instruct GPT" model provides superior outputs compared to the gpt-3 model with 175 billion parameters. While "instruct GPT" occasionally makes minor errors, this research suggests that fine-tuning with human feedback is a step in the right direction to make model outputs align more closely with human intent.


### äººç±»åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„è¯„ä¼°ï¼Œ
GPTï¼ˆpromptï¼‰ï¼šæ˜¯GPT3åœ¨promptä¸Šåšæ¯”è¾ƒå¤šçš„è°ƒæ•´

SFTï¼šç¬¬ä¸€ä¸ªæ¨¡å‹

PPOï¼šå½“Î³=0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPOï¼›

PPO-ptxï¼šå½“Î³ä¸ä¸º0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPO-ptxã€‚

![äººç±»åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„è¯„ä¼°ï¼Œ](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092703.png)


è¯­è¨€æ¨¡å‹ç»å¸¸ç¼–é€ äº‹å®ï¼Œç”Ÿæˆå¸¦æœ‰åè§å’Œæœ‰æ¯’çš„æ–‡æœ¬æˆ–è€…æ²¡æœ‰éµå¾ªç”¨æˆ·çš„æŒ‡ä»¤çš„åŸå› æ˜¯è¯­è¨€æ¨¡å‹çš„ç›®æ ‡å‡½æ•°ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtoken-è¿™ä¸æˆ‘ä»¬çš„ç›®æ ‡ä¸åŒï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç”Ÿæˆéµå¾ªç”¨æˆ·æŒ‡ä»¤çš„æœ‰ç”¨çš„å’Œå®‰å…¨çš„ç»“æœã€‚

The reason why language models often fabricate facts, generate biased or toxic text, or fail to follow user instructions is that the objective function of language models is designed for predicting the next token. This objective function does not align with our goals, which are to generate useful and safe results that adhere to user instructions.

ä½œè€…aligningè¯­è¨€æ¨¡å‹æ–¹æ³•ï¼ˆthree modelsï¼‰ï¼š

The authors propose an approach to align language models, which involves three models:


1)ä»äººç±»åé¦ˆä¸­ä½¿ç”¨å¢å¼ºå­¦ä¹ ï¼ˆRLHFï¼‰æ¥å¾®è°ƒGPT-3ï¼Œå…·ä½“çš„åšæ³•æ˜¯äººå·¥å†™äº†å¾ˆå¤špromptï¼Œç”¨æ ‡æ³¨å·¥å…·æŠŠç­”æ¡ˆå†™å‡ºæ¥ï¼Œè¿™æ ·å°±æ ‡æ³¨äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶åå¯¹GPT3æ¨¡å‹åšå¾®è°ƒï¼Œç»“æœæ˜¯SFTï¼ˆSupervised Fine-Tuning)ã€‚

1) They use Reinforcement Learning from Human Feedback (RLHF) to fine-tune GPT-3 based on human feedback. The specific process involves creating a dataset by manually writing numerous prompts and generating answers using an annotation tool. This dataset is used for supervised fine-tuning (SFT).


2) ä½¿ç”¨SFTæ¨¡å‹ï¼Œè¾“å‡ºå¤šä¸ªç»“æœï¼Œå¯¹ç»“æœè¿›è¡Œæ’åº(æ„å»ºæ–°çš„æ•°æ®é›†)ï¼Œè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼ˆäººå·¥æ ‡æ³¨å¤ªè´µï¼‰ã€‚

2) Using the SFT model, multiple responses are generated and ranked (constructing a new dataset), and a reward model (RM) is trained. Since manual annotation is costly, the RM helps assess the quality of responses.


3)ä½¿ç”¨RMä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œfine-tuneæœ‰ç›‘ç£å­¦ä¹ ï¼ˆSFTï¼‰ï¼Œä½¿ç”¨PPOç®—æ³•æ¥æœ€å¤§åŒ–å¥–åŠ±å‡½æ•°ã€‚è¿™ä¸ªæ¨¡å‹ç§°ä¸ºinstruct GPTã€‚

3) They employ the RM as a reward function and fine-tune the model using supervised learning (SFT) with the Proximal Policy Optimization (PPO) algorithm to maximize the reward function. This model is referred to as "instruct GPT."



![è¯­è¨€æ¨¡å‹æ–¹æ³•](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729092746.png)

ä½œè€…è®­ç»ƒäº†ä¸‰ä¸ªæ¨¡å‹ï¼ˆ1.3Bï¼Œ6Bï¼Œ175Bï¼‰ï¼Œè¿™äº›æ¨¡å‹éƒ½ä½¿ç”¨GPT-3æ¡†æ¶ã€‚

ä½œè€…å‘ç°å¦‚ä¸‹ï¼š

    ä¸GPT-3çš„è¾“å‡ºç›¸æ¯”ï¼Œæ ‡æ³¨è€…æ˜æ˜¾æ›´å–œæ¬¢InstructGPTçš„è¾“å‡º
    ä¸GPT-3ç›¸æ¯”ï¼ŒInstructGPTæ¨¡å‹çš„çœŸå®æ€§è¦å¥½ä¸€äº›ã€‚
    ä¸GPT-3ç›¸æ¯”ï¼ŒInstructGPTåœ¨æ¯’æ€§æ–¹é¢ç•¥æœ‰æ”¹å–„ï¼Œå› ä¸ºå®ƒå¯ä»¥è¯´æˆ‘ä¸æƒ³å›ç­”ä½ è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åè§ä¸Šæ²¡å¤ªå¤šæå‡ã€‚
    å¾®è°ƒçš„æ—¶å€™éƒ½æ˜¯é’ˆå¯¹æŸäº›ä»»åŠ¡åšå¾®è°ƒï¼Œå¯èƒ½ä½¿å¾—ä½ åœ¨ä¸€äº›åˆ«çš„ä»»åŠ¡ä¸Šæ€§èƒ½ä¼šä¸‹é™ã€‚
    æ²¡é€ è¿‡è®­ç»ƒæ•°æ®çš„é›‡å‘˜è§‰å¾—InstructGPTæ•ˆæœæ›´å¥½ï¼›
    å…¬å¼€çš„NLPæ•°æ®é›†ä¸èƒ½ååº”æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•è¢«ä½¿ç”¨çš„ï¼›
    InstructGPTæ¨¡å‹å±•ç¤ºäº†æ›´å¥½çš„æ³›åŒ–æ€§åœ¨é‚£äº›RLHFå¾®è°ƒä¹‹å¤–çš„æŒ‡ä»¤ä¸Šã€‚
    InstructGPTä»ç„¶ä¼šçŠ¯ä¸€äº›ç®€å•çš„é”™è¯¯ã€‚

The authors trained three models (1.3B, 6B, 175B), all based on the GPT-3 framework, and made the following observations:

- Compared to GPT-3's outputs, human labelers significantly preferred the outputs of InstructGPT.
- InstructGPT exhibited slightly better reliability compared to GPT-3.
- InstructGPT showed some improvement in toxicity relative to GPT-3 because it could respond by saying it didn't want to answer a particular question. However, there wasn't a significant reduction in bias.
- Fine-tuning for specific tasks might lead to performance drops on other tasks.
- Labelers who had not seen training data felt that InstructGPT's performance was better.
- Public NLP datasets might not reflect how language models are used in practice.
- InstructGPT demonstrated better generalization to instructions beyond those used in RLHF fine-tuning.
- InstructGPT still made some simple errors.


## ç›¸å…³å·¥ä½œ

    ä»äººçš„åé¦ˆä¸­å­¦ä¹ å’Œå¯¹é½
    ä½œè€…ä½¿ç”¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)è¿›è¡Œå¾®è°ƒ
    è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥éµå¾ªæŒ‡ä»¤ã€‚
    åœ¨å…¬å…±NPæ•°æ®é›†å¾®è°ƒï¼Œåœ¨ä¸åŒçš„NLPæ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚ç»“è®ºæ˜¯åœ¨ä¸€ç³»åˆ—NLPä»»åŠ¡ä¸Šå¾®è°ƒLMsï¼Œæé«˜äº†ä»–ä»¬åœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„è¡¨ç°ï¼Œæ— è®ºæ˜¯åœ¨zero-shotè¿˜æ˜¯few-shotä¸Šã€‚
    è¯„ä¼°è¯­è¨€æ¨¡å‹çš„å±å®³ï¼›
    æœ‰ä¸€ä¸ªæ–°å…´çš„ä¸æ–­å‘å±•çš„é¢†åŸŸï¼Œæ—¨åœ¨å»ºç«‹åŸºå‡†ï¼Œå…·ä½“è¯„ä¼°è¿™äº›å±å®³ï¼Œä½†å–å¾—è¿›å±•å¾ˆéš¾ã€‚
    ä¿®æ”¹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºä»¥å‡è½»å±å®³ã€‚
    åœ¨è¿‡æ»¤çš„æ•°æ®é›†ä¸Šè®­ç»ƒåï¼ŒLMsäº§ç”Ÿçš„æœ‰å®³æ–‡æœ¬è¾ƒå°‘ï¼Œä½†ä»£ä»·æ˜¯è¯­è¨€å»ºæ¨¡æ€§èƒ½ç•¥æœ‰ä¸‹é™ã€‚Xuä½¿ç”¨å¤šç§æ–¹æ³•æ¥æé«˜èŠå¤©æœºå™¨äººçš„å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬æ•°æ®è¿‡æ»¤ã€åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é˜»æ­¢æŸäº›å•è¯æˆ– n-gramã€æŒ‡å®štokenï¼Œ å’Œhuman-in-the-loop æ•°æ®æ”¶é›†ç­‰ã€‚


- Learning from Human Feedback and Alignment with Human Intent:
The authors used Reinforcement Learning from Human Feedback (RLHF) to fine-tune language models (LMs) to follow instructions.
- Fine-Tuning LMs on Public NLP Datasets:
The LMs were fine-tuned on public NLP datasets and evaluated on various NLP tasks, demonstrating improved performance in executing tasks in both zero-shot and few-shot settings.
- Evaluating the Harms of Language Models:
Evaluating the harms caused by language models is a challenging and evolving field, but efforts are being made to establish benchmarks for assessing these harms.
- Modifying LM Behavior to Mitigate Harms:
Training LMs on filtered datasets reduced the generation of harmful text but at the cost of slightly reduced language modeling performance. Various methods, including data filtering, blocking certain words or n-grams during generation, specifying tokens, and human-in-the-loop data collection, have been used to enhance the safety of chatbots, as demonstrated by Xu.


## æ–¹æ³•å’Œå®éªŒç»†èŠ‚

    ä½œè€…çš„æ–¹æ³•éµå¾ªäº†Ziegler et al.å’ŒStiennon et al.çš„æ–¹æ³•ï¼Œä»ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€ä¸€ä¸ªå¸Œæœ›æ¨¡å‹äº§ç”Ÿalignè¾“å‡ºçš„æç¤ºè¯­å’Œä¸€ç»„ç»è¿‡åŸ¹è®­çš„æ ‡æ³¨è€…å¼€å§‹ã€‚æ‰§è¡Œä¸‹é¢ä¸‰ä¸ªæ­¥éª¤
    Step1ï¼šæ”¶é›†æ•°æ®ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªç›‘ç£æ¨¡å‹ã€‚ä½¿ç”¨ç›‘ç£å­¦ä¹ åœ¨è¿™äº›æ•°æ®ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„GPT-3æ¨¡å‹ã€‚
    Step2ï¼šæ”¶é›†æ¯”è¾ƒæ•°æ®ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚æ”¶é›†ä¸€äº›æ¨¡å‹è¾“å‡ºä¹‹é—´çš„æ¯”è¾ƒæ•°æ®ï¼Œå…¶ä¸­æ ‡æ³¨è€…æŒ‡å‡ºä»–ä»¬å¯¹äºç»™å®šè¾“å…¥æ›´å–œæ¬¢å“ªä¸ªè¾“å‡ºã€‚ç„¶åè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹äººç±»åå¥½çš„è¾“å‡ºã€‚
    Step3ï¼šä½¿ç”¨PPOç®—æ³•ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ä¸‹çš„ç­–ç•¥ã€‚æŠŠå¥–åŠ±æ¨¡å‹çš„è¾“å‡ºä½œä¸ºä¸€ä¸ªæ ‡é‡å¥–åŠ±ï¼Œç”¨PPOç®—æ³•åœ¨è¿™ä¸ªå¥–åŠ±ä¸‹å¾®è°ƒç›‘ç£ç­–ç•¥ã€‚ç¬¬äºŒæ­¥å’Œç¬¬ä¸‰æ­¥å¯ä»¥ä¸æ–­è¿­ä»£ï¼›æ”¶é›†æ›´å¤šå½“å‰æœ€ä½³ç­–ç•¥ä¸‹çš„æ¯”è¾ƒæ•°æ®ï¼Œç”¨å®ƒä»¬æ¥è®­ç»ƒä¸€ä¸ªæ–°çš„å¥–åŠ±æ¨¡å‹å’Œä¸€ä¸ªæ–°çš„ç­–ç•¥ã€‚

The author's approach follows the methods of Ziegler et al. and Stiennon et al., starting with a pre-trained language model, a prompt intended to elicit aligned outputs, and a group of trained annotators. It involves three steps:

**Step 1:** Data Collection and Supervised Model Training
- Collect data and fine-tune a pre-trained GPT-3 model using supervised learning on this data.

**Step 2:** Comparative Data Collection and Reward Model Training
- Gather comparative data where annotators indicate their preference for the outputs given a specific input.
- Train a reward model to predict human-preferred outputs.

**Step 3:** Policy Optimization Using PPO Algorithm
- Utilize the output of the reward model as a scalar reward and fine-tune the supervised policy using the Proximal Policy Optimization (PPO) algorithm.
- Steps two and three can be iterated continuously: collect more comparative data under the current best policy to train a new reward model and a new policy.


--- 
    æç¤ºè¯­æ•°æ®é›†ä¸»è¦æ˜¯ç”±æäº¤åˆ°open AI API ä¸Šçš„æ–‡æœ¬æç¤ºè¯­ç»„æˆã€‚æ²¡æœ‰ä½¿ç”¨æ¥è‡ªä½¿ç”¨APIçš„å®¢æˆ·çš„æ•°æ®ï¼Œå¯¹æ•°æ®é›†åˆ é™¤é‡å¤çš„æç¤ºè¯ï¼Œé™åˆ¶æç¤ºè¯çš„é•¿åº¦ä¸º200ï¼Œè¿‡æ»¤è®­ç»ƒé›†ä¸­ä¸ªäººèº«ä»½ä¿¡æ¯ç­‰ã€‚è®­ç»ƒInstructGPTæ¨¡å‹æ—¶ï¼Œè¦æ±‚æ ‡æ³¨å‘˜å†™ä¸‹é¢ä¸‰ç±»æç¤ºè¯ï¼š
    Plain:æˆ‘ä»¬åªæ˜¯è¦æ±‚æ ‡æ³¨å‘˜å†™ä¸€ä¸ªä»»æ„çš„é—®é¢˜ï¼ŒåŒæ—¶ç¡®ä¿é—®é¢˜æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ã€‚ 
    Few-shot:æˆ‘ä»¬è¦æ±‚æ ‡æ³¨è€…æå‡ºä¸€ä¸ªæŒ‡ä»¤ï¼Œä»¥åŠè¯¥æŒ‡ä»¤çš„å¤šä¸ªæŸ¥è¯¢/å“åº”å¯¹ã€‚ 
    User-basedï¼šæˆ‘ä»¬åœ¨OpenAIçš„å€™é€‰åå•åº”ç”¨ç¨‹åºä¸­åˆ—å‡ºäº†è®¸å¤šç”¨ä¾‹APIã€‚æˆ‘ä»¬è¦æ±‚æ ‡æ³¨è€…æå‡ºä¸è¿™äº›ç”¨ä¾‹ç›¸å¯¹åº”çš„æç¤ºè¯ã€‚
    SFTæ•°æ®é›†ï¼š13Kæ¡æ•°æ®ï¼›RMæ•°æ®é›†ï¼š33Kæ¡æ•°æ®ï¼›PPOæ•°æ®é›†ï¼š31Kæ¡æ•°æ®

The prompt dataset primarily consists of text prompts submitted to the OpenAI API. It does not include data from customers using the API, and the dataset has undergone preprocessing steps such as removing duplicate prompts, limiting the length of prompts to 200 characters, and filtering out personal information in the training set. When training the InstructGPT model, annotators were required to provide prompts falling into three categories:

1. **Plain:** An arbitrary question is requested from the annotator, ensuring that the questions exhibit sufficient diversity.

2. **Few-shot:** An instruction prompt, along with multiple query/response pairs for that instruction, is requested.

3. **User-based:** Annotated prompts corresponding to the use cases listed in OpenAI's candidate applications.

The dataset statistics are as follows:
- SFT dataset: 13,000 entries.
- RM dataset: 33,000 entries.
- PPO dataset: 31,000 entries.

---

    è®­ç»ƒä»»åŠ¡æ¥è‡ªä¸¤ä¸ªæ¥æº:(1)ç”±æˆ‘ä»¬çš„æ ‡æ³¨è€…ç¼–å†™çš„æç¤ºæ•°æ®é›†å’Œï¼ˆ2ï¼‰åœ¨æˆ‘ä»¬çš„APIä¸Šæä¾›ç»™æ—©æœŸInstructGPTæ¨¡å‹çš„æç¤ºè¯­æ•°æ®é›†ã€‚
    æç¤ºè¯­éå¸¸å¤šæ ·åŒ–ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€é—®ç­”ã€å¯¹è¯ã€æ‘˜è¦ã€æ‘˜å½•ç­‰å…¶ä»–è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ•°æ®é›†è¶…è¿‡96%æ˜¯è‹±è¯­ã€‚
    å°½ç®¡ä»»åŠ¡å¾ˆå¤æ‚ï¼Œä½†æˆ‘ä»¬å‘ç°æ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´ç‡ç›¸å½“é«˜ï¼šè®­ç»ƒæ ‡æ³¨è€…åœ¨ 72.6 Â± 1.5% çš„æ—¶é—´å†…å½¼æ­¤ä¸€è‡´ï¼Œè€Œå¯¹äºhold-outè€…ï¼Œè¿™ä¸ªæ•°å­—æ˜¯ 77.3 Â± 1.3%ã€‚ä¸ºäº†æ¯”è¾ƒï¼Œç ”ç©¶äººå‘˜ä¹‹é—´çš„ä¸€è‡´æ€§ä¸º 73 Â± 4%ã€‚
    æˆ‘ä»¬ä»GPT-3é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¼€å§‹ã€‚æˆ‘ä»¬ç”¨ä¸‰ä¸ªä¸åŒçš„æŠ€æœ¯æ¥è®­ç»ƒæ¨¡å‹:
    Supervised fine-tuningï¼ˆSFTï¼‰ã€‚ç›‘ç£å¾®è°ƒ(SFT)ã€‚æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„æ ‡æ³¨æ•°æ®ä¸Šå¾®è°ƒGPT-3ä½¿ç”¨ç›‘ç£å­¦ã€‚æˆ‘ä»¬è®­ç»ƒäº†16ä¸ªepochï¼Œä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è¡°å‡ï¼Œæ®‹å·®ä¸º0.2ã€‚æˆ‘ä»¬æ ¹æ®éªŒè¯é›†ä¸Šçš„RMåˆ†æ•°è¿›è¡Œæœ€ç»ˆçš„SFTæ¨¡å‹é€‰æ‹©ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„SFTæ¨¡å‹åœ¨1 epochåçš„éªŒè¯é›†çš„æŸå¤±ä¸Šè¿‡æ‹Ÿåˆ;ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œä½†æ›´å¤šepochçš„è®­ç»ƒå¯¹RMåˆ†æ•°å’Œäººç±»åå¥½è¯„çº§éƒ½æœ‰å¸®åŠ©ã€‚

The training data for InstructGPT is sourced from two main channels: 

1. A dataset of prompt data created by our annotators.
2. A dataset of prompt data provided to the early InstructGPT models on our API.

The prompts are highly diverse, encompassing various natural language tasks, including generation, question-answering, dialogues, summaries, excerpts, and more. Over 96% of the dataset consists of prompts in English.

Despite the complexity of the tasks, the agreement among annotators is relatively high: training annotators exhibit approximately 72.6% Â± 1.5% mutual agreement, while for hold-out annotators, this figure is 77.3% Â± 1.3%. For comparison, inter-rater reliability among researchers is around 73% Â± 4%.

The training process initiates with the GPT-3 pretrained language model. Three distinct techniques are used to train the model:

1. **Supervised Fine-Tuning (SFT):** In SFT, GPT-3 is fine-tuned on our annotated data using supervised learning. Training is conducted over 16 epochs, employing cosine learning rate decay with a residual weight of 0.2. The final SFT model selection is based on RM scores from the validation set. It's observed that the SFT model tends to overfit on the validation loss after one epoch, but further training beyond this point is helpful for both RM scores and human preference ratings.


**Reward modelingï¼ˆRMï¼‰å¥–åŠ±å»ºæ¨¡(RM)**

SFTæ¨¡å‹å»æ‰æœ€åçš„umembedding layerï¼Œè¾“å…¥æç¤ºè¯­å’Œç­”æ¡ˆï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡ç»“æœï¼ˆrewardï¼‰ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­æˆ‘ä»¬ä»…ä½¿ç”¨äº†6B RMs, è¿™èŠ‚çœäº†å¾ˆå¤§çš„è®¡ç®—é‡ã€‚æˆ‘ä»¬å‘ç°175B RMè®­ç»ƒä¸ç¨³å®šå› æ­¤ä¸é€‚åˆç”¨äºå€¼å‡½æ•°ï¼Œåœ¨RLæœŸé—´ã€‚
å¥–åŠ±æ¨¡å‹çš„æŸå¤±å‡½æ•°æ˜¯ï¼š

The loss function for the reward model (RM) in the context of InstructGPT is as follows:

The RM's loss function comprises two primary components:

1. **RM Cross-Entropy Loss:** This component measures the agreement between the RM's predicted probabilities and the human ranker's preferences. It is calculated using the cross-entropy loss between the predicted probabilities assigned to different model outputs and the actual human preferences provided in the dataset.

2. **Rank-Corrected Loss:** This component is designed to ensure that the RM assigns higher scores to responses that are ranked more favorably by humans. It helps in penalizing the RM when its ranking does not match the human preferences.

The combination of these components in the loss function helps the reward model learn to predict responses that align with human preferences, which is crucial for reinforcement learning from human feedback (RLHF). This methodology is used to fine-tune the language model and make its output more in line with user intent.

![å¥–åŠ±å»ºæ¨¡(RM)](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093037.png)


r_ğœƒ (ğ‘¥,ğ‘¦)æ˜¯å¥–åŠ±æ¨¡å‹å¯¹äºæç¤ºè¯xå’Œå®Œæˆyçš„æ ‡é‡è¾“å‡ºï¼Œy_wæ˜¯y_wå’Œy_lä¸­æ›´å—æ¬¢è¿çš„è¡¥å…¨ï¼ŒDæ˜¯äººç±»æ¯”è¾ƒçš„æ•°æ®é›†ã€‚
ä¸ºäº†åŠ å¿«æ¯”è¾ƒæ”¶é›†ï¼Œå–K = 9ï¼Œ36å¯¹æ’åºè¿›è¡Œä¼˜åŒ–

**Reinforcement learning (RL)  å¼ºåŒ–å­¦ä¹ (RL)**

ä½œè€…ä½¿ç”¨PPOæ¥å¾®è°ƒSFTæ¨¡å‹ã€‚è¾“å…¥ä¸€ä¸ªpromptæœŸæœ›å¾—åˆ°ä¸€ä¸ªè¾“å‡ºã€‚ç»™å®šä¸€ä¸ªpromptå’Œresponseï¼Œç”Ÿæˆå¥–åŠ±åˆ†æ•°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¢åŠ äº†KLæ•£åº¦é™ä½å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªæ¨¡å‹ä¸ºPPOã€‚
ä½œè€…æŠŠé¢„è®­ç»ƒçš„æ¢¯åº¦åŠ å…¥åˆ°PPOçš„æ¢¯åº¦ä¸­ï¼Œä¸ºäº†ç¼“å’Œæ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸­çš„æ€§èƒ½æŸå¤±ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªæ¨¡å‹ä¸ºPPO-ptxã€‚
æˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æœ€å¤§åŒ–è¿™ä¸ªç›®æ ‡å‡½æ•°ã€‚

The authors use the Proximal Policy Optimization (PPO) algorithm to fine-tune the Supervised Fine-Tuned (SFT) model, which is referred to as the PPO model. The PPO model aims to maximize the following objective:

1. **Expected Reward Maximization:** In this step, given a prompt and response, the model generates a reward score. The PPO model seeks to maximize the expected reward by optimizing its policy to produce responses that align with human preferences.

To mitigate potential performance losses of the model on public datasets during reinforcement learning, the authors combine the pre-trained gradients with the PPO gradients. This model is referred to as PPO-ptx, where "ptx" represents the incorporation of the pre-trained gradients. The objective in both cases is to maximize expected reward by optimizing the model's policy through reinforcement learning.

![å¼ºåŒ–å­¦ä¹ (RL)](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093148.png)

**Baselinesï¼š**

æˆ‘ä»¬å¯¹PPOæ¨¡å‹ã€SFTæ¨¡å‹å’ŒGPT-3çš„æ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬ä¹Ÿå¯¹æ¯”äº†ç»™gpt-3çš„æç¤ºè¯æä¾›few-shot å‰ç¼€çš„æ¨¡å‹ã€‚å‰ç¼€é™„åŠ åœ¨ç”¨æˆ·æŒ‡å®šçš„æŒ‡ä»¤ä¹‹å‰ã€‚

è¿˜å¯¹æ¯”äº†InstructGPTä¸åœ¨FLANå’ŒT0ä¸Šçš„å¾®è°ƒ175B GPT-3æ¨¡å‹ã€‚ä¸¤ä¸ªæ•°æ®é›†éƒ½ç”±å„ç§NLPä»»åŠ¡ç»„æˆï¼Œç»“åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚æˆ‘ä»¬åˆ†åˆ«åœ¨å¤§çº¦100ä¸‡ä¸ªä¾‹å­ä¸Šå¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒå¹¶é€‰æ‹©åœ¨éªŒè¯é›†ä¸Šè·å¾—æœ€é«˜å¥–åŠ±æ¨¡å‹åˆ†æ•°é€‰ä¸ºcheckpointã€‚

è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦‚ä½•â€œå¯¹é½â€çš„ï¼Œå®šé‡è¯„ä¼°åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„éƒ¨åˆ†

    APIè¯„ä»·ï¼šä¸»è¦çš„è¯„ä»·æ ‡å‡†æ˜¯äººç±»åå¥½èƒœç‡
    å¯¹å…¬å…±NLPæ•°æ®é›†çš„è¯„ä¼°

We compared the performance of the PPO model, the SFT model, and GPT-3. We also compared models that used few-shot prefixes for the prompts given to GPT-3. These prefixes are added before the user-specified instructions.

We further compared InstructGPT to the fine-tuned 175B GPT-3 models on the FLAN and T0 datasets. Both datasets consist of various NLP tasks combined with natural language instructions. We fine-tuned them separately on approximately one million examples and selected the models with the highest reward model scores on the validation set as checkpoints.

The evaluation of our models on how well they "align" is quantitatively assessed in two independent parts:

1. **API Evaluation:** The primary evaluation criterion is the human preference win rate.
2. **Evaluation on Public NLP Datasets:** This evaluation involves assessing model performance on common NLP datasets.

## ç»“æœ

    å›¾3:æˆ‘ä»¬çš„æ¨¡å‹çš„åå¥½ç»“æœï¼Œé€šè¿‡å¯¹æ¯”175B SFTæ¨¡å‹çš„èƒœç‡ã€‚
    æˆ‘ä»¬ä»å¯¹æäº¤ç»™GPT-3æ¨¡å‹(å·¦)çš„æç¤ºçš„è¯„ä¼°ä¸­çœç•¥äº†GPT(prompt) 
    å› ä¸ºè¿™äº›æç¤ºå·²ç»è®¾è®¡å¾—å¾ˆå¥½ï¼Œå¯ä»¥é€‚ç”¨Â äºGPT-3ï¼Œ å’ŒInstructGPT æ¨¡å‹å¾—æç¤ºæ°å¥½ç›¸åï¼ˆå³ï¼‰ã€‚

![ç»“æœ](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093256.png)

    å›¾4ä¸­ï¼Œå±•ç¤ºäº†æ ‡æ³¨è€…ä¹Ÿå¯¹InstructGPTè¾“å‡ºè¿›è¡Œäº†æ›´å…·ä½“çš„è¯„ä»·è½´ã€‚å…·ä½“æ¥è¯´ï¼Œä¸GPT-3ç›¸æ¯”ï¼ŒInstructGPT è¾“å‡ºæ›´æ¥è¿‘ä¸€ä¸ªå®¢æˆ·åŠ©ç†ï¼Œæ›´ç»å¸¸éµå¾ªæŒ‡ä»¤ä¸­æ˜ç¡®å®šä¹‰çš„çº¦æŸï¼Œæ›´å°‘çš„æœ‰å®³æ€§ã€‚

![ç»“æœ1](https://cdn.jsdelivr.net/gh/1oscar/image_house@main/20230729093424.png)



## è®¨è®º

    åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„å¯¹é½ç ”ç©¶æ–¹æ³•æ˜¯è¿­ä»£çš„:æˆ‘ä»¬æ­£åœ¨æ”¹è¿›çš„æ˜¯ç°æœ‰çš„AIç³»ç»Ÿçš„å¯¹é½è€Œä¸æ˜¯æŠ½è±¡åœ°ä¸“æ³¨äºè°ƒæ•´å°šä¸å­˜åœ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚
    ä½œè€…åœ¨ç ”ç©¶ä¸­å¸å–çš„ç»éªŒæ•™è®­ï¼š
        å¢åŠ æ¨¡å‹å¯¹é½çš„æˆæœ¬ç›¸å¯¹äºé¢„è®­ç»ƒæ˜¯æ›´åˆ’ç®—çš„ã€‚
        æˆ‘ä»¬å·²ç»çœ‹åˆ°ä¸€äº›è¯æ®è¡¨æ˜ï¼ŒInstructGPTå°†â€œéµå¾ªæŒ‡ä»¤â€æ³›åŒ–åˆ°æˆ‘ä»¬ä¸å¯¹å…¶è¿›è¡Œç›‘ç£çš„è®¾ç½®ï¼Œä¾‹å¦‚éè‹±è¯­è¯­è¨€ä»»åŠ¡å’Œä¸ä»£ç ç›¸å…³çš„ä»»åŠ¡ã€‚
        æˆ‘ä»¬èƒ½å¤Ÿå‡è½»é€šè¿‡å¾®è°ƒå¸¦æ¥çš„å¤§å¤šæ•°æ€§èƒ½ä¸‹é™ã€‚
        æˆ‘ä»¬å·²ç»ä»ç°å®ä¸–ç•Œçš„ç ”ç©¶ä¸­éªŒè¯äº†å¯¹é½æŠ€æœ¯ã€‚

In this work, our approach to alignment research is iterative. We are focused on improving existing AI systems' alignment rather than abstractly tuning AI systems that do not yet exist.

Here are some key lessons and insights the authors have learned in the course of this research:

1. **Increasing alignment in post-training is more cost-effective than during pre-training.**
   
2. There is some evidence that InstructGPT generalizes "following instructions" to settings where it is not supervised, such as non-English language tasks and code-related tasks.

3. Most of the performance degradation introduced by fine-tuning can be mitigated.

4. Alignment techniques have been validated in real-world research.

---

    å±€é™æ€§
    InstructGPTæ¨¡å‹çš„è¡Œä¸ºéƒ¨åˆ†æ˜¯ç”±äººç±»çš„åé¦ˆå†³å®šçš„ï¼ˆæ ‡æ³¨å‘˜ï¼‰ã€‚ä¸€äº›æ ‡ç­¾ä»»åŠ¡ä¾èµ–äºä»·å€¼åˆ¤æ–­ï¼Œè¿™å¯èƒ½æ˜¯å—æˆ‘ä»¬æ‰¿åŒ…å•†çš„èº«ä»½ï¼Œä¿¡ä»°ï¼Œæ–‡åŒ–èƒŒæ™¯å’Œä¸ªäººç»å†çš„å½±å“ã€‚ 
    æˆ‘ä»¬çš„æ¨¡å‹æ—¢ä¸å®Œå…¨å¯¹é½ï¼Œä¹Ÿä¸å®Œå…¨å®‰å…¨;ä»–ä»¬ä»ç„¶äº§ç”Ÿæœ‰æ¯’æˆ–åè§è¾“å‡ºï¼Œç¼–é€ äº‹å®ï¼Œåœ¨æ²¡æœ‰æ˜ç¡®æç¤ºçš„æƒ…å†µä¸‹äº§ç”Ÿæ€§å’Œæš´åŠ›å†…å®¹ã€‚
    å¼€æ”¾é—®é¢˜
    å¯ä»¥ä½¿ç”¨å¯¹æŠ—è®¾ç½®æ¥å‡å°‘æœ‰å®³çš„è¾“å‡ºï¼›
    æ¯”è¾ƒä¹Ÿä¸ä¸€å®šæ˜¯æä¾›å¯¹é½ä¿¡å·çš„æœ€æœ‰æ•ˆæ–¹å¼ã€‚
    å¦ä¸€ä¸ªå¯èƒ½æ”¹è¿›æˆ‘ä»¬æ–¹æ³•çš„ä¿®æ”¹æ˜¯è¿‡æ»¤é¢„è®­ç»ƒæ··åˆæ•°æ®ä¸­çš„æœ‰æ¯’å†…å®¹æˆ–ä½¿ç”¨åˆæˆæŒ‡ä»¤æ‰©å……æ­¤æ•°æ®ã€‚

Limitations:
- The behavior of the InstructGPT model is partly determined by human feedback (annotators), and some tasks rely on value judgments that may be influenced by the identity, beliefs, cultural background, and personal experiences of the contractors.
- Our models are neither fully aligned nor completely safe; they still produce toxic or biased outputs, fabricate facts, and generate sexual or violent content without explicit prompts.

Open Questions:
- Adversarial setups can potentially reduce harmful outputs.
- Comparison may not necessarily be the most effective way to provide alignment signals.
- Another potential improvement to our approach is to filter out toxic content from the pretraining mixture data or use synthetic prompts to augment this data.

## é™„å½•

### æ ‡æ³¨å‘˜å†™æç¤ºè¯çš„ä¸‰ç§ç±»å‹
Plainï¼šæˆ‘ä»¬åªæ˜¯è¦æ±‚æ ‡è®°è€…æå‡ºä¸€ä¸ªä»»æ„ä»»åŠ¡ï¼ŒåŒæ—¶ç¡®ä¿ä»»åŠ¡çš„å¤šæ ·æ€§ã€‚
Few-shotï¼šæˆ‘ä»¬è¦æ±‚æ ‡æ³¨è€…æå‡ºä¸€æ¡æŒ‡ä»¤ï¼Œä»¥åŠè¯¥æŒ‡ä»¤çš„å¤šä¸ªæŸ¥è¯¢/å“åº”å¯¹ã€‚
User-basedï¼šæˆ‘ä»¬åœ¨ OpenAI API çš„åº”ç”¨ç¨‹åºä¸­é™ˆè¿°äº†è®¸å¤šç”¨ä¾‹ã€‚ æˆ‘ä»¬è¦æ±‚æ ‡æ³¨è€…æå‡ºä¸è¿™äº›ç”¨ä¾‹ç›¸å¯¹åº”çš„æç¤ºã€‚

### Three Types of Prompts Provided by Annotators
Plain: Annotators were asked to come up with any task, ensuring diversity in the tasks suggested.
Few-shot: Annotators were tasked with providing an instruction and multiple query/response pairs for that instruction.
User-based: Annotators were required to suggest prompts corresponding to the various use cases presented in the OpenAI API application.


### API ç”¨æˆ·æç¤ºè¯
ä½¿ç”¨Instruct GPTæ¨¡å‹æ—©æœŸç‰ˆæœ¬ç”¨æˆ·æäº¤çš„æç¤ºè¯ã€‚ä¸ºäº†ä¿è¯æç¤ºè¯çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬ä¼šåˆ é™¤é‡å¤çš„æç¤ºè¯ï¼Œé™åˆ¶æç¤ºè¯çš„é•¿åº¦ä¸º200ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åŸºäºç»„ç»‡idæ¥åˆ’åˆ†è®­ç»ƒé›†ï¼Œæµ‹è¯•å’ŒéªŒè¯é›†ã€‚
å›¾è¡¨å±•ç¤ºäº†ä¸€äº›ç”¨æˆ·çš„promptsï¼šç”Ÿæˆ(generation)ã€å¼€æ”¾å¼(open) QAã€å°é—­å¼(closed) QAã€å¤´è„‘é£æš´ã€èŠå¤©(chat)ã€é‡å†™ã€æ€»ç»“ã€åˆ†ç±»ã€æå–æˆ–å…¶ä»–ã€‚
Table6 å±•ç¤ºäº†ç”¨äºè®­ç»ƒ/éªŒè¯ SFTã€RM å’Œ RL æ¨¡å‹çš„æ•°æ®é›†çš„å¤§å°
Table7å±•ç¤ºäº†æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚
Table8å±•ç¤ºäº†æ¯ä¸ªç”¨æˆ·çš„å¹³å‡promptsæ•°é‡ï¼›
Table9å±•ç¤ºäº†æ•°æ®é›†åˆ†ç±»çš„æç¤ºè¯é•¿åº¦ï¼Œç±»åˆ«åˆ†ç±»çš„æç¤ºè¯é•¿åº¦

### API User Prompts
User prompts were drawn from early versions of the Instruct GPT model submitted by users. To ensure diversity in prompts, duplicates were removed, and the length of prompts was limited to 200 characters. Additionally, the training, test, and validation sets were organized based on organization IDs.

The figures show some user prompts in different categories: generation, open QA, closed QA, brainstorming, chat, rewriting, summarization, classification, extraction, and others.

Table 6 presents the sizes of the datasets used for training/validation of SFT, RM, and RL models.
Table 7 illustrates the diversity of the datasets.
Table 8 shows the average number of prompts per user.
Table 9 provides information on prompt length distribution by category.


### æ¨¡å‹ç»†èŠ‚

#### Training Details for All Models
All models were built using the GPT-3 architecture and trained using the Adam optimizer.



#### æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨äº†GPT-3æ¶æ„ã€‚æ‰€æœ‰æ¨¡å‹è®­ç»ƒä½¿ç”¨Adamä¼˜åŒ–å™¨ã€‚
1.SFTè®­ç»ƒç»†èŠ‚
SFTæ¨¡å‹è®­ç»ƒ16ä¸ªepochï¼Œdropoutæ˜¯0.2ï¼Œå­¦ä¹ ç‡æ˜¯åŸå§‹å€¼çš„10%ï¼Œæ²¡æœ‰å­¦ä¹ ç‡é¢„çƒ­ã€‚1.3Bå’Œ6Bæ¨¡å‹ï¼Œä½¿ç”¨LRå€¼9.65e-6ï¼Œbatchsize=32ï¼›175Bä½¿ç”¨LRå€¼5.03e-6ï¼Œbatchsize=8ã€‚æœ€ç»ˆæ¨¡å‹æ ¹æ®RMåˆ†æ•°è¿›è¡Œé€‰æ‹©çš„ã€‚

1. **SFT Training Details**
   - SFT models were trained for 16 epochs.
   - Dropout rate was set to 0.2.
   - The learning rate started at 10% of the base value and had no learning rate warm-up.
   - For 1.3B and 6B models, the learning rate was 9.65e-6, and the batch size was 32.
   - For the 175B model, the learning rate was 5.03e-6, and the batch size was 8.
   - The final model was chosen based on RM scores.

#### RMè®­ç»ƒç»†èŠ‚
é€‰æ‹©6B RMæ¨¡å‹çš„åŸå› ï¼šå¤§æ¨¡å‹ä¸ç¨³å®šã€‚RMæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹epochæ•æ„Ÿï¼Œå¤šä¸ªepochå¾ˆå¿«å°±è¿‡æ‹Ÿåˆï¼Œè¿™é‡Œä½œè€…åªè®­ç»ƒäº†ä¸€ä¸ªepochï¼Œå­¦ä¹ ç‡9e-6ï¼Œbatchsize=64ã€‚æ¯ä¸ªprompté€‰æ‹©k=4å’Œk=9æ ‡ç­¾è¡¥å…¨ï¼Œç»“æœä¼šæœ‰(KÂ¦2)ä¸ªå¯¹æ¯”é¡¹ï¼Œå› æ­¤å•æ‰¹æœ€å¤šå¯ä»¥åŒ…å«64*(KÂ¦2)â‰¤2304 ä¸ªå¯¹æ¯”é¡¹ã€‚

2. **RM Training Details**
   - The 6B RM model was chosen due to the instability of larger models.
   - RM models were trained for one epoch.
   - The learning rate was set to 9e-6 with a batch size of 64.
   - Each prompt had k=4 and k=9 label completions, leading to (K choose 2) comparison items in each batch, allowing for a maximum of 2304 comparison items per batch.

#### RLHFæ¨¡å‹çš„åˆå§‹åŒ–ç»†èŠ‚
ä»é¢„è®­ç»ƒçš„GPT-3æ¨¡å‹åˆå§‹åŒ–RLHFæ¨¡å‹ï¼Œåœ¨æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒ 2ä¸ªepochã€‚
å¾®è°ƒæœŸé—´æ··å…¥äº†10%çš„é¢„è®­ç»ƒæ•°æ®ï¼Œå› ä¸ºå‘ç°è¿™å¯¹PPOè®­ç»ƒå¾ˆæœ‰å¸®åŠ©ã€‚
ä½¿ç”¨äº†1.3Bå’Œ6Bçš„batchsize=32ï¼›175Bçš„batchsize=8ã€‚ä½œè€…å¯¹æ¯”äº†æ¯ä¸ªæ¨¡å‹çš„ä¸åŒçš„å³°å€¼çš„å­¦ä¹ ç‡ï¼Œé€‰æ‹©äº†åœ¨æ¼”ç¤ºå’Œé¢„è®­ç»ƒéªŒè¯é›†ä¸ŠæŸå¤±éƒ½æ¯”è¾ƒå°çš„é‚£ä¸ªã€‚

3. **Initialization Details for RLHF Models**
   - The RLHF models were initialized from the pre-trained GPT-3 model and supervised fine-tuned for 2 epochs on the dataset.
   - During fine-tuning, 10% of the pre-training data was mixed in as it was found to be helpful for PPO training.
   - For 1.3B and 6B models, batch sizes were set to 32. For 175B, the batch size was 8.
   - Various peak learning rates were tested for each model, and the one with the smallest loss on the demo and pre-training validation sets was chosen.


#### RLHFè®­ç»ƒçš„ç»†èŠ‚
æ‰€æœ‰çš„PPOæ¨¡å‹ä½¿ç”¨6B RMå’Œ 6Bå€¼å‡½æ•°ã€‚å€¼å‡½æ•°1.3Bå’Œ6Bçš„å­¦ä¹ ç‡æ˜¯9e-6ï¼Œ175Bçš„å­¦ä¹ ç‡æ˜¯5e-6ï¼›
é¢„è®­ç»ƒç¤ºä¾‹æ˜¯ RL è®­ç»ƒé›†æ•°çš„ 8 å€ï¼Œ Â é¢„è®­ç»ƒæ•°æ®æ˜¯ä»ç”¨äºè®­ç»ƒ GPT-3 æ¨¡å‹çš„æ•°æ®é›†ä¸­éšæœºæŠ½å–çš„ã€‚
å¯¹äºæ¯ä¸ªbatchï¼Œæˆ‘ä»¬åœ¨è¿ç»­çš„æ­¥éª¤ä¸­è®¡ç®— PPO æ¢¯åº¦å’Œé¢„è®­ç»ƒæ¢¯åº¦ï¼Œå¹¶å°†å®ƒä»¬éƒ½ç´¯ç§¯åˆ°æ¢¯åº¦ç¼“å†²åŒºä¸­ã€‚
æˆ‘ä»¬å°†é¢„è®­ç»ƒæ¢¯åº¦ä¹˜ä»¥ä¸€ä¸ªç³»æ•° Î³ = 27.8ï¼Œä»¥æ§åˆ¶æ¥è‡ª PPO å’Œé¢„è®­ç»ƒåˆ†å¸ƒçš„æ¢¯åº¦çš„ç›¸å¯¹å¼ºåº¦ã€‚

4. **Training Details for RLHF**
   - All PPO models used the 6B RM and value functions.
   - The learning rate for value functions was 9e-6 for 1.3B and 6B models, and 5e-6 for the 175B model.
   - The pre-training dataset was eight times the size of the RL training set, randomly sampled from the dataset used to train the GPT-3 model.
   - For each batch, PPO and pre-training gradients were computed in consecutive steps and accumulated in a gradient buffer.
   - The pre-training gradient was scaled by a factor Î³ = 27.8 to control the relative strength of gradients from PPO and pre-training distributions.


#### FLANå’ŒT0æ¨¡å‹
 æˆ‘ä»¬é€šè¿‡åœ¨ FLAN å’Œ T0 æ•°æ®é›†ä¸Šå¾®è°ƒ 175B GPT-3 æ¨¡å‹æ¥è·å¾—æˆ‘ä»¬çš„ FLAN å’Œ T0 åŸºçº¿ã€‚ å°† T0 æ•°æ®é›†ä¸‹é‡‡æ ·åˆ° 1Mä¸ªæ•°æ®ç‚¹ï¼Œä»¥ä½¿æ¯ä¸ªæ¨¡å‹çš„è®­ç»ƒæ•°æ®é‡å…·æœ‰å¯æ¯”æ€§ã€‚
FLAN checkpointï¼šä½¿ç”¨6B RMæ¨¡å‹å¯¹promptéªŒè¯é›†è¿›è¡Œè¯„åˆ†ã€‚æœ€åé€‰æ‹©äº†4e-6 çš„å­¦ä¹ ç‡å’Œ 896k ä¸ªç¤ºä¾‹è¿›è¡Œè®­ç»ƒçš„checkpointã€‚
T0 checkpoiintã€‚åœ¨ä¸€ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†batchå¤§å°ä¸º128ï¼Œå­¦ä¹ ç‡ä¸º4e-6ï¼Œæ ·æœ¬æ•°ä¸º128ä¸‡ã€‚å¦ä¸€ä¸ªå®éªŒä½¿ç”¨æ‰¹å¤§å°ä¸º64ï¼Œå­¦ä¹ ç‡ä¸º6e-6ï¼Œæ ·æœ¬æ•°ä¸º100ä¸‡ã€‚å†æ¬¡ä½¿ç”¨å¥–åŠ±æ¨¡å‹åˆ†æ•°ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ ç‡ä¸º4e-6 ,896kä¸ªè®­ç»ƒæ ·æœ¬åä»å‰ä¸€ä¸ªå®éªŒä¸­é€‰æ‹©äº†ã€‚


5. **FLAN and T0 Models**
   - The FLAN baseline was obtained by fine-tuning the 175B GPT-3 model on the FLAN dataset.
   - The T0 baseline was achieved by fine-tuning on the T0 dataset, downsampled to 1 million data points, making training data comparable.
   - FLAN checkpoint: A learning rate of 4e-6 and 896k examples were used for training.
   - T0 checkpoint: Two experiments were conducted, one with a batch size of 128, a learning rate of 4e-6, and 1.28 million samples. The other used a batch size of 64, a learning rate of 6e-6, and 1 million samples. A model checkpoint was selected from the first experiment based on RM scores, using a learning rate of 4e-6 and 896k training samples.


## æˆ‘çš„ç‚¹è¯„
è‡ªä»chatgpt3å¤§ç«ä¹‹åï¼Œaiç»˜ç”»ä¹Ÿå¼€å§‹çˆ†ç«ã€‚å¾ˆå¤šå›¢é˜Ÿç„å‡†äº†è¿™ä¸ªæ–¹å‘å»åˆ›ä¸šã€‚è¿™ç¯‡æ–‡ç« ç®—æ˜¯æ¯”è¾ƒç»å…¸çš„æ–‡ç”Ÿå›¾çš„èŒƒä¾‹ï¼Œå¾ˆå€¼å¾—ç»†ç»†ç ”ç©¶é˜…è¯»ã€‚

Since the rise of ChatGPT-3, AI-generated art has also gained popularity. Many teams are venturing into this field for entrepreneurial opportunities. This article can be considered a classic example of text-to-image generation and is certainly worth a closer examination and study.

